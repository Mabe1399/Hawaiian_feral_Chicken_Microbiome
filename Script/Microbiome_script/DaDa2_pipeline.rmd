# DADA2 Pipeline for taxonomy assignement

This is the actual amplicon sequence inference, it allow use to create a table of amplicon sequence variants (ASV).

The pipeline can be divided in multiple part:

- Read Quality Check
- Filter and Trim (if necessary)
- Check The Error Rate
- Sample Inference
- Merge Paired Reads (if working with paired reads)
- Construct Sequence Table
- Remove Chimeras
- Track Reads For Quality Check
- Assign Taxonomy


Because we did some data preparation before we need to first load the workspace used:

```{r, eval = F}
# Load the previous workspace
load("/Users/matiasbeckerburgos/Desktop/Master_in_Bioinformatics/Master_Thesis/Hawaiian_feral_Chicken_Microbiome/Backup_workspace/Data_preparation.RData")
```

But also load the packages needed in this analysis and load the data.

```{r , eval = F}
# In this case only DADA2 package
library(dada2)

# Create lists of file directory for forward and reverse reads.

fnFs <- sort(list.files(cut_output, pattern = "R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(cut_output, pattern = "R2_001.fastq.gz", full.names = TRUE))

# Sample names  have been extracted before so no need to do it.
```

Now it is time to start.

## Read Quality Check

First we need to visualize the read quality. (In the previous part, we already do it using fastqc but it is always good to do it)

```{r, eval = F}
plotQualityProfile(fnFs[1:2]) # visualize only the two first reads
plotQualityProfile(fnRs[1:2])
```

After visualizing think about a strategy if the quality is not good enough.

## Filter and Trim 

This part is optional depending on the quality of your reads.

First, create objects containing the directory were the output data would be kept (the directory doesn't need to exist)

```{r, eval = F}
filtFs <- file.path(path, "Filtered/RUN1", paste0(sample.names, "_F_filt.fastq.gz"))

filtRs <- file.path(path, "Filtered/RUN1", paste0(sample.names, "_R_filt.fastq.gz"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Now the actual fitering and trimming

In the tutorial, they advise basic parameters has: maxN=0, truncQ=2, rm.phix=TRUE and maxEE=2

Parameter definition:

`maxN`: Default 0. After truncation, sequences with more than maxN Ns will be discarded. Note that dada does not allow Ns

`truncQ`: Default 2. Truncate reads at the first instance of a quality score less than or equal to truncQ.

`rm.phix`: Default TRUE. If TRUE, discard reads that match against the phiX genome, as determined by isPhiX

`maxEE`: Default Inf (no EE filtering). After truncation, reads with higher than maxEE "expected errors" will be discarded. Expected errors are calculated from the nominal definition of the quality score: EE = sum(10^(-Q/10))

`truncLen`: Default 0 (no truncation). Truncate reads after truncLen bases. Reads shorter than this are discarded

```{r, eval = F }
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,
    truncLen = c(260, 270),
    maxN = 0, maxEE = c(2, 2), truncQ = 2,
    rm.phix = TRUE, compress = TRUE, multithread = TRUE
)

head(out)
```

## Check The Error Rate

We can evaluate the error rate and visualize it.

```{r, eval = F, fig.show="hold", out.width="50%"}
errF <- learnErrors(filtFs, multithread = T)
errR <- learnErrors(filtRs, multithread = T)


par(mfrow = c(1, 2)) # Set for two plots

plotErrors(errF, nominalQ = T)
plotErrors(errR, nominalQ = T)
```

This is mostly informative in case something went wrong

## Sample inference

We apply the sample inference algorithm to the data. 

(maybe consider pooling for increase sensitivity ?)

```{r, eval = F}
dadaFs <- dada(filtFs, err = errF, multithread = T)

dadaRs <- dada(filtRs, err = errF, multithread = T)

# Inspect the dada-class object
dadaFs[[1]]
```

## Merge Paired Reads

This step merge all our samples in one data frame

```{r, eval = F}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = T)

# Inspect the merger data.frame
head(mergers[[1]])
```

## Construct Sequence Table

This the actual step constructing a sequence table

```{r, eval = F}
# ASV table, higher-resolution version of the OTU table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

## Merge the dataset

In this step because we have two Dataset we merge them for downstream analysis.

```{r, eval = F}
# Combine the seqtab from the two runs
Combined_seqtab <- mergeSequenceTables(seqtab_Blanks, seqtab.nochim, repeats = "sum")
```

## Remove Chimeras

This is a important step

```{r , eval = F}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = T, verbose = T)

# Check dimension
dim(seqtab.nochim)

# Check chimera frequency
sum(seqtab.nochim) / sum(seqtab)
```

After removing chimera, you should still have a lot of reads (most of them) if it is not the case check upstream steps.

## Track Reads for Quality Check

Check the number of read who made it through the pipeline.

```{r, eval = F}
getN <- function(x) sum(getUniques(x))
track <- cbind(
    out, sapply(dadaFs, getN), sapply(dadaRs, getN),
    sapply(mergers, getN), rowSums(seqtab.nochim)
)

colnames(track) <- c(
    "input", "filtered", "denoisedF",
    "denoisedR", "merged", "nonchim"
)

rownames(track) <- sample.names
head(track)

# Export Reads Tracking through pipeline into CSV
write.csv(track, file = file.path(output, "DADA2/Hawaiian_Chicken_DADA2_track_reads.csv"))
```

## Assign Taxonomy

The most important step of the analysis we need to assign taxonomy to the ASV

The default classifier is naive Bayesian classifier but other classifier can be used.

multiple database are available (Silva, etc..)

```{r, eval= F}
Database <- file.path(output, "DADA2", "Database")
taxa <- assignTaxonomy(seqtab.nochim, file.path(Database, "silva_nr99_v138.2_toSpecies_trainset.fa.gz"), multithread = T)
# taxa <- addSpecies(taxa, file.path(Database, "silva_v138.2_assignSpecies.fa.gz"))


# Inspect the taxonomic assignement
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)

# Check percentage of assignement by level.

stat <- as.matrix(100 - colSums(is.na(taxa.print)) / 19585 * 100)
colnames(stat) <- "Percentage"

stat
```

## Save the important object for further analysiy

```{r, eval = F}
# Erase all objects except seqtab.nochim and taxa
rm(list = ls()[!(ls() %in% c("seqtab.nochim", "taxa", "save"))])

# Save the workspace
save.image(file.path(save, "DaDa2_pipeline_RUN1.RData"))
```