# DADA2 Pipeline for taxonomy assignement

This is the actual amplicon sequence inference, it allow use to create a table of amplicon sequence variants (ASV).

The pipeline can be divided in multiple part:

- Read Quality Check
- Filter and Trim (if necessary)
- Check The Error Rate
- Sample Inference
- Merge Paired Reads (if working with paired reads)
- Construct Sequence Table
- Remove Chimeras
- Track Reads For Quality Check
- Assign Taxonomy


Because we did some data preparation before we need to first load the workspace used:

```{r, eval = F}
# Load the previous workspace
load("/Users/matiasbeckerburgos/Desktop/Master_in_Bioinformatics/Master_Thesis/Hawaiian_feral_Chicken_Microbiome/Backup_workspace/Data_preparation.RData")
```

But also load the packages needed in this analysis and load the data.

```{r , eval = F}
# In this case only DADA2 package
library(dada2)

# Create lists of file directory for forward and reverse reads.

fnFs <- sort(list.files(cut_output, pattern = "R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(cut_output, pattern = "R2_001.fastq.gz", full.names = TRUE))

# Sample names  have been extracted before so no need to do it.
```

Now it is time to start.

## Read Quality Check

First we need to visualize the read quality. (In the previous part, we already do it using fastqc but it is always good to do it)

```{r, eval = F}
plotQualityProfile(fnFs[1:2])# visualize only the two first reads
plotQualityProfile(fnRs[1:2])
```

After visualizing think about a strategy if the quality is not good enough.

## Filter and Trim 

This part is optional depending on the quality of your reads.

First, create objects containing the directory were the output data would be kept (the directory doesn't need to exist)

```{r, eval = F}

filtFs <- file.path(path, "Filtered", paste0(sample.names, "_F_filt.fastq.gz"))

filtRs <- file.path(path, "Filtered", paste0(sample.names, "_R_filt.fastq.gz"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names

```

Now the actual fitering and trimming

In the tutorial, they advise basic parameters has: maxN=0, truncQ=2, rm.phix=TRUE and maxEE=2

Parameter definition:

`maxN`:

`truncQ`:

`rm.phix`:

`maxEE`:

`truncLen`: 

```{r, eval = F }
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = c(260,270), 
                            maxN = 0, maxEE = c(2,2), truncQ = 2, 
                            rm.phix=TRUE, compress = TRUE, multithread = TRUE)

head(out)
```

## Check The Error Rate

We can evaluate the error rate and visualize it.

```{r, eval = F, fig.show="hold", out.width="50%"}
errF <- learnErrors(filtFs, multithread = T)
errR <- learnErrors(filtRs, multithread = T)


par(mfrow = c(1,2))# Set for two plots

plotErrors(errF, nominalQ = T)
plotErrors(errR, nominalQ = T)

```

This is mostly informative in case something went wrong

## Sample inference

We apply the sample inference algorithm to the data. 

(maybe consider pooling for increase sensitivity ?)

```{r, eval = F}

dadaFs <- dada(filtFs, err=errF, multithread= T)

dadaRs <- dada(filtRs, err=errF, multithread = T)

#Inspect the dada-class object
dadaFs[[1]]
```

## Merge Paired Reads

This step merge all our samples in one data frame

```{r, eval = F}

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose= T)

# Inspect the merger data.frame
head(mergers[[1]])
```

## Construct Sequence Table

This the actual step constructing a sequence table

```{r, eval = F}
# ASV table, higher-resolution version of the OTU table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

## Remove Chimeras

This is a important step

```{r , eval = F}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread= T, verbose = T)

# Check dimension
dim(seqtab.nochim)

# Check chimera frequency
sum(seqtab.nochim)/sum(seqtab)
```

After removing chimera, you should still have a lot of reads (most of them) if it is not the case check upstream steps.

## Track Reads for Quality Check

Check the number of read who made it through the pipeline.

```{r, eval = F}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs,getN), sapply(dadaRs, getN), 
                sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", 
                        "denoisedR", "merged", "nonchim")

rownames(track) <- sample.names
head(track)
```

## Assign Taxonomy

The most important step of the analysis we need to assign taxonomy to the ASV

The default classifier is naive Bayesian classifier but other classifier can be used.

multiple database are available (Silva, etc..)

```{r, eval= F}
Database <- file.path(output, "DADA2", "Database")
taxa <- assignTaxonomy(seqtab.nochim, file.path(Database, "silva_nr99_v138.1_train_set copy.fa.gz"), multithread = T)

# Inspect the taxonomic assignement
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)
```

Alternative, Use the Decipher taxonomic assignent.

```{r}
# Load Decipher package
library(DECIPHER)


# Use the classifier
dna <- readDNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs
load("~/DADA2/Database/SILVA_SSU_r132_March2018.RData") # CHANGE TO THE PATH OF YOUR TRAINING SET
ids <- IdTaxa(dna, trainingSet, strand="top", processors=NULL, verbose=FALSE) # use all processors
ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species") # ranks of interest
# Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
taxid <- t(sapply(ids, function(x) {
        m <- match(ranks, x$rank)
        taxa <- x$taxon[m]
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))
colnames(taxid) <- ranks; rownames(taxid) <- getSequences(seqtab.nochim)
```

## Save the important object for further analysiy

```{r, eval = F}
# Erase all objects except seqtab.nochim and taxa
keep(seqtab.nochim, taxa, save, sure = T)

# Save the workspace
save.image(file.path(save,"DaDa2_pipeline.RData"))
```